{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCwxoE3kwWLaVRs4RdbaNe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanesaHM/ProyectoKaggle/blob/main/03_modelo_con_preprocesado_(KNN_%2B_OHE_%2B_RobustScaler)_y_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UDEA/ai4eng 20252 - Pruebas Saber Pro Colombia**\n",
        "\n",
        "Crear un modelo para predecir el rendimiento de los estudiantes en las pruebas Saber Pro"
      ],
      "metadata": {
        "id": "5G8Cr83a-mRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descripción general**\n",
        "\n",
        "Las Pruebas Saber Pro son exámenes estandarizados que se administran en Colombia para evaluar la calidad y el nivel de conocimiento y competencias de los estudiantes de educación superior, es decir, de instituciones de educación superior como universidades y tecnológicos. Estas pruebas son parte de los esfuerzos del Gobierno de Colombia para monitorear y mejorar la calidad de la educación superior en el país.\n",
        "\n",
        "Estas Pruebas constan cinco componentes genéricos, Inglés, Lectura Crítica, Competencias Ciudadanas, Razonamiento Cuantitativo y Comunicación Escrita.\n",
        "\n",
        "Tu tarea será crear un modelo de clasificación que para cada estudiante prediga qué desempeño va a tener: bajo, medio-bajo, medio-alto o alto."
      ],
      "metadata": {
        "id": "hmMM0_ze-7Js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descripción**\n",
        "\n",
        "El conjunto de datos contiene más de 50 columnas que describen de manera distintos aspectos de cada estudiante, incluyendo:\n",
        "\n",
        "Información socieconómica: Describen características socieconómicas del estudiante como su estrato, educación de sus padres, estrato, entre otras.\n",
        "\n",
        "Información de instituciones: Describen las instituciones de donde provienen los estudiantes.\n",
        "\n",
        "Información del estudiante: Describe particularidades del estudiante como su edad, que programa estudian, la modalidad de estudio, etc.\n",
        "\n",
        "Información estadística: Describe algunos coeficientes que equipos de estudio han desarrollado que podría ayudar a la clasificación.\n",
        "\n",
        "Así como muchos otros datos que ayudan a clasificar de manera precisa los niveles de desempeño"
      ],
      "metadata": {
        "id": "zemYOiZe_Rt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo con preprocesado  (KNN + OHE + RobustScaler) y XGBoost**\n",
        "\n"
      ],
      "metadata": {
        "id": "73F_K802SKfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Importar librerías**"
      ],
      "metadata": {
        "id": "qxkxGgLfBau0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsxYTe_M-W6k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler, LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Configuración visual**"
      ],
      "metadata": {
        "id": "4unuiIErBmcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\")"
      ],
      "metadata": {
        "id": "3y_7ELf0BrQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Cargar los datos**"
      ],
      "metadata": {
        "id": "mCLCJ-2SB2gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "eauNGNJSUufQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n",
        "!chmod 600 ./kaggle.json\n",
        "!kaggle competitions download -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia"
      ],
      "metadata": {
        "id": "34zmsVf0B4SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Descomprimir los datos**"
      ],
      "metadata": {
        "id": "Uf19pAiiCSZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip udea*.zip > /dev/null"
      ],
      "metadata": {
        "id": "T-BrCfI9Cati"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test  = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "hxSykEUtCw8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANÁLISIS EXPLORATORIO INICIAL**\n",
        "\n",
        "Se muestran las primeras filas, tipos y valores nulos."
      ],
      "metadata": {
        "id": "DDa3yhzXDGBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train is not None:\n",
        "    display(train.head())\n",
        "    display(train.info())\n",
        "    display(train.isna().sum().sort_values(ascending=False).head(20))\n",
        "else:\n",
        "    print('No train available to display.')"
      ],
      "metadata": {
        "id": "BcoLv-_SWUD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPROCESAMIENTO**"
      ],
      "metadata": {
        "id": "vpPimPFNd1yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_processed = train.copy()\n",
        "inicio_total = time.time()\n",
        "\n",
        "ID_COL = \"ID\"\n",
        "TARGET = \"RENDIMIENTO_GLOBAL\"\n",
        "NUM_VARS = ['PERIODO_ACADEMICO', 'INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']\n",
        "ESTRATO_MAP = {f'Estrato {i}': i for i in range(1, 7)}\n",
        "REVERSE_ESTRATO = {v: k for k, v in ESTRATO_MAP.items()}\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "SAMPLE_FRAC = 0.5\n",
        "\n",
        "print(f\"\\nFilas originales: {len(train_processed):,}\")\n",
        "\n",
        "# SAMPLING\n",
        "if len(train_processed) > 200000:\n",
        "    print(f\"Stratified sampling al {SAMPLE_FRAC*100:.0f}%...\")\n",
        "    train_sample, _ = train_test_split(\n",
        "        train_processed,\n",
        "        train_size=SAMPLE_FRAC,\n",
        "        random_state=RANDOM_STATE,\n",
        "        stratify=train_processed[TARGET]\n",
        "    )\n",
        "    train_processed = train_sample.reset_index(drop=True)\n",
        "    print(f\"Filas después: {len(train_processed):,}\")\n",
        "\n",
        "# LIMPIEZA\n",
        "cols_drop = [c for c in train_processed.columns if \".1\" in c]\n",
        "train_processed.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "\n",
        "cat_vars = [c for c in train_processed.columns if c not in NUM_VARS + [ID_COL, TARGET]]\n",
        "print(f\"Numéricas: {len(NUM_VARS)} | Categóricas: {len(cat_vars)}\")\n",
        "\n",
        "# IMPUTACIÓN F_ESTRATOVIVIENDA\n",
        "if \"F_ESTRATOVIVIENDA\" in train_processed.columns:\n",
        "    socio_vars = [\n",
        "        'F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR', 'F_TIENELAVADORA',\n",
        "        'F_TIENEINTERNET', 'F_EDUCACIONPADRE', 'F_EDUCACIONMADRE',\n",
        "        'E_VALORMATRICULAUNIVERSIDAD'\n",
        "    ]\n",
        "    socio_vars = [v for v in socio_vars if v in train_processed.columns]\n",
        "\n",
        "    temp = train_processed[socio_vars + [\"F_ESTRATOVIVIENDA\"]].copy()\n",
        "    for col in socio_vars:\n",
        "        temp[col] = temp[col].astype(\"category\").cat.codes.replace({-1: np.nan})\n",
        "\n",
        "    temp[\"F_ESTRATOVIVIENDA\"] = temp[\"F_ESTRATOVIVIENDA\"].map(ESTRATO_MAP)\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
        "    imputado = np.round(imputer.fit_transform(temp)[:, -1]).clip(1, 6)\n",
        "    train_processed[\"F_ESTRATOVIVIENDA\"] = pd.Series(imputado, index=train_processed.index).map(REVERSE_ESTRATO)\n",
        "\n",
        "    del temp, imputer, imputado\n",
        "\n",
        "# IMPUTACIÓN CATEGÓRICAS\n",
        "for col in cat_vars:\n",
        "    if train_processed[col].isnull().sum() > 0:\n",
        "        fill_value = (\n",
        "            train_processed[col].mode()[0]\n",
        "            if train_processed[col].nunique() <= 3\n",
        "            else \"Desconocido\"\n",
        "        )\n",
        "        train_processed[col] = train_processed[col].fillna(fill_value)\n",
        "\n",
        "print(\"Imputación completada\")\n",
        "\n",
        "# OUTLIERS\n",
        "for col in NUM_VARS:\n",
        "    p1, p99 = train_processed[col].quantile([0.01, 0.99])\n",
        "    train_processed[col] = train_processed[col].clip(p1, p99)\n",
        "\n",
        "print(\"Outliers cappeados\")\n",
        "\n",
        "# TRAIN/TEST SPLIT\n",
        "X = train_processed[NUM_VARS + cat_vars]\n",
        "y = train_processed[TARGET]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "del train_processed, X, y\n",
        "print(f\"Train/Test split: {len(X_train):,} / {len(X_test):,}\")\n",
        "\n",
        "# TRANSFORMACIONES\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", RobustScaler(), NUM_VARS),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, max_categories=50), cat_vars),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "cat_ohe_cols = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(cat_vars)\n",
        "final_cols = NUM_VARS + list(cat_ohe_cols)\n",
        "\n",
        "print(f\"Features finales: {len(final_cols)}\")\n",
        "\n",
        "# CODIFICAR TARGET\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "print(f\"Clases codificadas: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
        "\n",
        "# BALANCEO\n",
        "class_counts = np.bincount(y_train_encoded)\n",
        "imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "\n",
        "if imbalance_ratio > 2.5:\n",
        "    print(f\"Aplicando SMOTE (ratio: {imbalance_ratio:.2f})...\")\n",
        "    smote = SMOTE(\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        k_neighbors=3,\n",
        "        sampling_strategy='not majority'\n",
        "    )\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(\n",
        "        X_train_processed,\n",
        "        y_train_encoded\n",
        "    )\n",
        "else:\n",
        "    print(f\"Balanceo aceptable (ratio: {imbalance_ratio:.2f})\")\n",
        "    X_train_balanced, y_train_balanced = X_train_processed, y_train_encoded\n",
        "\n",
        "print(\"\\nPREPROCESAMIENTO COMPLETADO\\n\")\n"
      ],
      "metadata": {
        "id": "SrNjMoNG4PB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENTRENAMIENTO**"
      ],
      "metadata": {
        "id": "9x7z-Hcq0DG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMODELO: XGBoost Classifier\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ENTRENAR\n",
        "print(\"\\nEntrenando XGBoost...\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced, verbose=False)\n",
        "print(\"Entrenamiento completado.\")\n",
        "\n",
        "# VALIDACIÓN CRUZADA\n",
        "print(\"\\nValidación cruzada (3-fold)...\")\n",
        "cv_scores = cross_val_score(\n",
        "    xgb_model,\n",
        "    X_train_balanced,\n",
        "    y_train_balanced,\n",
        "    cv=3,\n",
        "    scoring='f1_weighted'\n",
        ")\n",
        "print(f\"CV F1-Weighted: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "# PREDICCIONES\n",
        "print(\"\\nGenerando predicciones en TEST...\")\n",
        "y_pred_xgb = xgb_model.predict(X_test_processed)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test_processed)\n",
        "\n",
        "# MÉTRICAS\n",
        "acc_xgb = accuracy_score(y_test_encoded, y_pred_xgb)\n",
        "f1_xgb = f1_score(y_test_encoded, y_pred_xgb, average='weighted')\n",
        "\n",
        "print(\"\\nRESULTADOS XGBoost:\")\n",
        "print(f\"  Accuracy:  {acc_xgb:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_xgb:.4f}\")\n",
        "\n",
        "print(\"\\nREPORTE DE CLASIFICACIÓN:\")\n",
        "print(classification_report(y_test_encoded, y_pred_xgb, target_names=le.classes_, digits=4))\n",
        "\n",
        "# MATRIZ DE CONFUSIÓN\n",
        "cm = confusion_matrix(y_test_encoded, y_pred_xgb)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    ax=ax,\n",
        "    xticklabels=le.classes_,\n",
        "    yticklabels=le.classes_\n",
        ")\n",
        "ax.set_title('Matriz de Confusión - XGBoost')\n",
        "ax.set_ylabel('Real')\n",
        "ax.set_xlabel('Predicción')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# IMPORTANCIA DE FEATURES\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': final_cols,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTOP 15 FEATURES MÁS IMPORTANTES:\")\n",
        "print(feature_importance.head(15).to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "feature_importance.head(15).sort_values('Importance').plot(\n",
        "    kind='barh',\n",
        "    x='Feature',\n",
        "    y='Importance',\n",
        "    ax=ax,\n",
        "    color='steelblue'\n",
        ")\n",
        "ax.set_title('Top 15 Features - XGBoost', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# GUARDAR MODELO\n",
        "print(\"\\nGuardando modelo y preprocesadores...\")\n",
        "with open(\"xgboost_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(xgb_model, f)\n",
        "with open(\"preprocessor_xgb.pkl\", \"wb\") as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "with open(\"label_encoder_xgb.pkl\", \"wb\") as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "print(\"Entrenamiento finalizado.\")"
      ],
      "metadata": {
        "id": "Qg7ssRzlQSi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREDICCIÓN EN DATASET COMPLETO**"
      ],
      "metadata": {
        "id": "KwP_TAprQsoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCargando dataset original sin samplear...\")\n",
        "test_proc = test.copy()\n",
        "\n",
        "print(f\"Test original shape: {test_proc.shape}\")\n",
        "\n",
        "# PREPROCESAR TEST\n",
        "cols_drop = [c for c in test_proc.columns if \".1\" in c]\n",
        "test_proc.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "\n",
        "if \"F_ESTRATOVIVIENDA\" in test_proc.columns:\n",
        "    socio_vars = [v for v in socio_vars if v in test_proc.columns]\n",
        "    temp = test_proc[socio_vars + [\"F_ESTRATOVIVIENDA\"]].copy()\n",
        "\n",
        "    for col in socio_vars:\n",
        "        temp[col] = temp[col].astype(\"category\").cat.codes.replace({-1: np.nan})\n",
        "\n",
        "    temp[\"F_ESTRATOVIVIENDA\"] = temp[\"F_ESTRATOVIVIENDA\"].map(ESTRATO_MAP)\n",
        "\n",
        "    from sklearn.impute import KNNImputer\n",
        "    imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
        "    imputado = np.round(imputer.fit_transform(temp)[:, -1]).clip(1, 6)\n",
        "\n",
        "    test_proc[\"F_ESTRATOVIVIENDA\"] = (\n",
        "        pd.Series(imputado, index=test_proc.index).map(REVERSE_ESTRATO)\n",
        "    )\n",
        "\n",
        "    del temp, imputer, imputado\n",
        "\n",
        "# IMPUTACIÓN VARIABLES CATEGÓRICAS\n",
        "for col in cat_vars:\n",
        "    if test_proc[col].isnull().sum() > 0:\n",
        "        fill_value = (\n",
        "            test_proc[col].mode()[0]\n",
        "            if test_proc[col].nunique() <= 3\n",
        "            else \"Desconocido\"\n",
        "        )\n",
        "        test_proc[col] = test_proc[col].fillna(fill_value)\n",
        "\n",
        "# WINSORIZACIÓN / LIMITAR EXTREMOS\n",
        "for col in NUM_VARS:\n",
        "    p1, p99 = test_proc[col].quantile([0.01, 0.99])\n",
        "    test_proc[col] = test_proc[col].clip(p1, p99)\n",
        "\n",
        "print(\"Test preprocesado.\")\n",
        "\n",
        "# TRANSFORMAR\n",
        "X_test_full = test_proc[NUM_VARS + cat_vars]\n",
        "X_test_full_processed = preprocessor.transform(X_test_full)\n",
        "\n",
        "print(f\"Transformaciones aplicadas: {X_test_full_processed.shape}\")\n",
        "\n",
        "# PREDECIR\n",
        "print(f\"\\nRealizando predicciones en {len(X_test_full):,} muestras...\")\n",
        "y_pred_encoded = xgb_model.predict(X_test_full_processed)\n",
        "y_pred = le.inverse_transform(y_pred_encoded)\n",
        "\n",
        "print(\"Predicciones completadas.\")"
      ],
      "metadata": {
        "id": "9dxLWudkQpur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREAR SUBMISSION**"
      ],
      "metadata": {
        "id": "LEd3q19aQ27T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'ID': test_proc[ID_COL].values,\n",
        "    'RENDIMIENTO_GLOBAL': y_pred\n",
        "})\n",
        "\n",
        "print(\"\\nDistribución de predicciones:\")\n",
        "for cls in le.classes_:\n",
        "    count = (y_pred == cls).sum()\n",
        "    pct = count / len(y_pred) * 100\n",
        "    print(f\"  {cls}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# GUARDAR ARCHIVO\n",
        "print(\"\\nGuardando submission_xgboost.csv...\")\n",
        "submission.to_csv('submission_xgboost.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Archivo generado: submission_xgboost.csv\")\n",
        "print(f\"Filas: {len(submission):,}\")\n",
        "print(f\"Columnas: {list(submission.columns)}\")\n",
        "print(\"\\n\" + submission.head(10).to_string(index=False))\n",
        "\n",
        "tiempo_total = time.time() - inicio_total\n",
        "print(f\"\\nTiempo total: {tiempo_total/60:.2f} minutos\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "ynLPqkCbQ0Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia -f submission_xgboost.csv -m \"XGBoost\""
      ],
      "metadata": {
        "id": "HZx_r-LlFcpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}