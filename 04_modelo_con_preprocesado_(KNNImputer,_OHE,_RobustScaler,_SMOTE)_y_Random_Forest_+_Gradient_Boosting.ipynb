{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn4pPVQRx0pdqjkTKIOdho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanesaHM/ProyectoKaggle/blob/main/04_modelo_con_preprocesado_(KNNImputer%2C_OHE%2C_RobustScaler%2C_SMOTE)_y_Random_Forest_%2B_Gradient_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UDEA/ai4eng 20252 - Pruebas Saber Pro Colombia**\n",
        "\n",
        "Crear un modelo para predecir el rendimiento de los estudiantes en las pruebas Saber Pro"
      ],
      "metadata": {
        "id": "5G8Cr83a-mRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descripción general**\n",
        "\n",
        "Las Pruebas Saber Pro son exámenes estandarizados que se administran en Colombia para evaluar la calidad y el nivel de conocimiento y competencias de los estudiantes de educación superior, es decir, de instituciones de educación superior como universidades y tecnológicos. Estas pruebas son parte de los esfuerzos del Gobierno de Colombia para monitorear y mejorar la calidad de la educación superior en el país.\n",
        "\n",
        "Estas Pruebas constan cinco componentes genéricos, Inglés, Lectura Crítica, Competencias Ciudadanas, Razonamiento Cuantitativo y Comunicación Escrita.\n",
        "\n",
        "Tu tarea será crear un modelo de clasificación que para cada estudiante prediga qué desempeño va a tener: bajo, medio-bajo, medio-alto o alto."
      ],
      "metadata": {
        "id": "hmMM0_ze-7Js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descripción**\n",
        "\n",
        "El conjunto de datos contiene más de 50 columnas que describen de manera distintos aspectos de cada estudiante, incluyendo:\n",
        "\n",
        "Información socieconómica: Describen características socieconómicas del estudiante como su estrato, educación de sus padres, estrato, entre otras.\n",
        "\n",
        "Información de instituciones: Describen las instituciones de donde provienen los estudiantes.\n",
        "\n",
        "Información del estudiante: Describe particularidades del estudiante como su edad, que programa estudian, la modalidad de estudio, etc.\n",
        "\n",
        "Información estadística: Describe algunos coeficientes que equipos de estudio han desarrollado que podría ayudar a la clasificación.\n",
        "\n",
        "Así como muchos otros datos que ayudan a clasificar de manera precisa los niveles de desempeño"
      ],
      "metadata": {
        "id": "zemYOiZe_Rt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **modelo con preprocesado (KNNImputer, OHE, RobustScaler, SMOTE) y Random Forest + Gradient Boosting**\n",
        "\n"
      ],
      "metadata": {
        "id": "73F_K802SKfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Importar librerías**"
      ],
      "metadata": {
        "id": "qxkxGgLfBau0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsxYTe_M-W6k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler, StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Configuración visual**"
      ],
      "metadata": {
        "id": "4unuiIErBmcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\")"
      ],
      "metadata": {
        "id": "3y_7ELf0BrQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Cargar los datos**"
      ],
      "metadata": {
        "id": "mCLCJ-2SB2gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "eauNGNJSUufQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n",
        "!chmod 600 ./kaggle.json\n",
        "!kaggle competitions download -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia"
      ],
      "metadata": {
        "id": "34zmsVf0B4SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Descomprimir los datos**"
      ],
      "metadata": {
        "id": "Uf19pAiiCSZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip udea*.zip > /dev/null"
      ],
      "metadata": {
        "id": "T-BrCfI9Cati"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test  = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "hxSykEUtCw8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANÁLISIS EXPLORATORIO INICIAL**\n",
        "\n",
        "Se muestran las primeras filas, tipos y valores nulos."
      ],
      "metadata": {
        "id": "DDa3yhzXDGBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train is not None:\n",
        "    display(train.head())\n",
        "    display(train.info())\n",
        "    display(train.isna().sum().sort_values(ascending=False).head(20))\n",
        "else:\n",
        "    print('No train available to display.')"
      ],
      "metadata": {
        "id": "BcoLv-_SWUD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPROCESAMIENTO**"
      ],
      "metadata": {
        "id": "vpPimPFNd1yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURACIÓN\n",
        "\n",
        "train_processed = train.copy()\n",
        "inicio = time.time()\n",
        "\n",
        "ID_COL = \"ID\"\n",
        "TARGET = \"RENDIMIENTO_GLOBAL\"\n",
        "NUM_VARS = ['PERIODO_ACADEMICO', 'INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']\n",
        "ESTRATO_MAP = {f'Estrato {i}': i for i in range(1, 7)}\n",
        "REVERSE_ESTRATO = {v: k for k, v in ESTRATO_MAP.items()}\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "SAMPLE_FRAC = 0.5\n",
        "\n",
        "print(\"\\n=== PREPROCESAMIENTO PARA ~700K FILAS ===\")\n",
        "print(f\"Filas originales: {len(train_processed):,}\")\n",
        "\n",
        "# SAMPLING ESTRATIFICADO\n",
        "if len(train_processed) > 200000:\n",
        "    print(f\"\\nAplicando stratified sampling al {SAMPLE_FRAC*100:.0f}%...\")\n",
        "    train_sample, _ = train_test_split(\n",
        "        train_processed, train_size=SAMPLE_FRAC, random_state=RANDOM_STATE,\n",
        "        stratify=train_processed[TARGET]\n",
        "    )\n",
        "    train_processed = train_sample.reset_index(drop=True)\n",
        "    print(f\"Filas después del sampling: {len(train_processed):,}\")\n",
        "\n",
        "# 1. LIMPIEZA RÁPIDA\n",
        "\n",
        "cols_drop = [c for c in train_processed.columns if \".1\" in c]\n",
        "train_processed.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "\n",
        "cat_vars = [c for c in train_processed.columns if c not in NUM_VARS + [ID_COL, TARGET]]\n",
        "\n",
        "print(f\"\\nNuméricas: {NUM_VARS}\")\n",
        "print(f\"Categóricas: {len(cat_vars)}\")\n",
        "\n",
        "# 2. IMPUTACIÓN F_ESTRATOVIVIENDA\n",
        "\n",
        "if \"F_ESTRATOVIVIENDA\" in train_processed.columns:\n",
        "    print(\"\\nImputando F_ESTRATOVIVIENDA...\")\n",
        "    socio_vars = [\n",
        "        'F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR', 'F_TIENELAVADORA',\n",
        "        'F_TIENEINTERNET', 'F_EDUCACIONPADRE', 'F_EDUCACIONMADRE',\n",
        "        'E_VALORMATRICULAUNIVERSIDAD'\n",
        "    ]\n",
        "    socio_vars = [v for v in socio_vars if v in train_processed.columns]\n",
        "\n",
        "    temp = train_processed[socio_vars + [\"F_ESTRATOVIVIENDA\"]].copy()\n",
        "    for col in socio_vars:\n",
        "        temp[col] = temp[col].astype(\"category\").cat.codes.replace({-1: np.nan})\n",
        "\n",
        "    temp[\"F_ESTRATOVIVIENDA\"] = temp[\"F_ESTRATOVIVIENDA\"].map(ESTRATO_MAP)\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
        "    imputado = np.round(imputer.fit_transform(temp)[:, -1]).clip(1, 6)\n",
        "    train_processed[\"F_ESTRATOVIVIENDA\"] = pd.Series(imputado, index=train_processed.index).map(REVERSE_ESTRATO)\n",
        "\n",
        "    del temp, imputer, imputado\n",
        "\n",
        "# 3. IMPUTACIÓN CATEGÓRICAS\n",
        "\n",
        "print(\"Imputando categóricas...\")\n",
        "for col in cat_vars:\n",
        "    if train_processed[col].isnull().sum() > 0:\n",
        "        fill_value = train_processed[col].mode()[0] if train_processed[col].nunique() <= 3 else \"Desconocido\"\n",
        "        train_processed[col].fillna(fill_value, inplace=True)\n",
        "\n",
        "# 4. OUTLIERS - CAPPING\n",
        "\n",
        "print(\"Capping outliers...\")\n",
        "for col in NUM_VARS:\n",
        "    p1, p99 = train_processed[col].quantile([0.01, 0.99])\n",
        "    train_processed[col] = train_processed[col].clip(p1, p99)\n",
        "\n",
        "# 5. ANÁLISIS CORRELACIÓN\n",
        "\n",
        "le_temp = LabelEncoder()\n",
        "target_encoded = le_temp.fit_transform(train_processed[TARGET])\n",
        "\n",
        "print(\"\\nCorrelaciones con TARGET:\")\n",
        "vars_to_keep = []\n",
        "for col in NUM_VARS:\n",
        "    corr = np.corrcoef(train_processed[col], target_encoded)[0, 1]\n",
        "    if abs(corr) > 0.05:\n",
        "        print(f\"{col}: {corr:.4f}\")\n",
        "        vars_to_keep.append(col)\n",
        "    else:\n",
        "        print(f\"{col}: {corr:.4f} (removida)\")\n",
        "\n",
        "NUM_VARS = vars_to_keep if vars_to_keep else NUM_VARS\n",
        "\n",
        "# 6. SEPARAR TRAIN/TEST\n",
        "print(\"\\nTrain/Test split...\")\n",
        "X = train_processed[NUM_VARS + cat_vars]\n",
        "y = train_processed[TARGET]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "del train_processed, X, y\n",
        "print(f\"Train: {len(X_train):,}, Test: {len(X_test):,}\")\n",
        "\n",
        "# 7. TRANSFORMACIÓN PIPELINE\n",
        "print(\"Ajustando transformaciones...\")\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", RobustScaler(), NUM_VARS),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, max_categories=50), cat_vars)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "cat_ohe_cols = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(cat_vars)\n",
        "final_cols = NUM_VARS + list(cat_ohe_cols)\n",
        "\n",
        "print(f\"Features finales: {len(final_cols)}\")\n",
        "\n",
        "# 8. CODIFICAR TARGET\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "print(f\"Classes: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
        "\n",
        "# 9. GUARDAR\n",
        "print(\"\\nGuardando archivos...\")\n",
        "\n",
        "df_train = pd.DataFrame(X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed, columns=final_cols)\n",
        "df_train[TARGET] = y_train.values\n",
        "df_train.to_parquet(\"train_processed.parquet\", compression=\"snappy\")\n",
        "\n",
        "df_test = pd.DataFrame(X_test_processed.toarray() if hasattr(X_test_processed, 'toarray') else X_test_processed, columns=final_cols)\n",
        "df_test[TARGET] = y_test.values\n",
        "df_test.to_parquet(\"test_processed.parquet\", compression=\"snappy\")\n",
        "\n",
        "with open(\"preprocessor.pkl\", \"wb\") as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "with open(\"data_for_training.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        'X_train': X_train_processed,\n",
        "        'X_test': X_test_processed,\n",
        "        'y_train': y_train_encoded,\n",
        "        'y_test': y_test_encoded,\n",
        "        'le': le,\n",
        "        'final_cols': final_cols\n",
        "    }, f)\n",
        "\n",
        "# RESUMEN\n",
        "tiempo_total = time.time() - inicio\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PREPROCESAMIENTO COMPLETADO\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Tiempo total: {tiempo_total/60:.2f} minutos\")\n",
        "print(f\"Train: {len(df_train):,} | Test: {len(df_test):,}\")\n",
        "print(f\"Features: {len(final_cols)}\")\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "dqWdJwKBrTOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENTRENAMIENTO**"
      ],
      "metadata": {
        "id": "9x7z-Hcq0DG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGAR DATOS PREPROCESADOS\n",
        "\n",
        "print(\"Cargando datos preprocesados...\\n\")\n",
        "\n",
        "with open(\"data_for_training.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train']\n",
        "X_test = data['X_test']\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']\n",
        "le = data['le']\n",
        "final_cols = data['final_cols']\n",
        "\n",
        "print(f\"Datos cargados: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "print(f\"Classes: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
        "\n",
        "# ANALISIS DE BALANCEO\n",
        "print(\"\\nDistribución de clases:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for cls, cnt in zip(unique, counts):\n",
        "    print(f\"  {le.classes_[cls]}: {cnt:,} ({cnt/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "class_counts = np.bincount(y_train)\n",
        "imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}\")\n",
        "\n",
        "# APLICAR SMOTE SI NECESARIO\n",
        "if imbalance_ratio > 2.5:\n",
        "    print(\"\\nAplicando SMOTE...\")\n",
        "    smote = SMOTE(random_state=42, n_jobs=-1, k_neighbors=3, sampling_strategy='not majority')\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "    print(f\"Train balanceado: {len(X_train_balanced):,} muestras\")\n",
        "else:\n",
        "    print(\"\\nSMOTE omitido (balanceo aceptable)\")\n",
        "    X_train_balanced, y_train_balanced = X_train, y_train\n",
        "\n",
        "# 1. RANDOM FOREST\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODELO RANDOM FOREST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100, max_depth=15, max_features='sqrt',\n",
        "    random_state=42, n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nValidación cruzada (3-fold)...\")\n",
        "cv_scores = cross_val_score(rf, X_train_balanced, y_train_balanced, cv=3, scoring='f1_weighted')\n",
        "print(f\"CV F1-Weighted: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "print(\"\\nEntrenando modelo completo...\")\n",
        "rf.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"\\nPrediciendo en TEST...\")\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_proba_rf = rf.predict_proba(X_test)\n",
        "\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "prec_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "rec_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "print(\"\\nResultados Random Forest:\")\n",
        "print(f\"  Accuracy:  {acc_rf:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_rf:.4f}\")\n",
        "print(f\"  Precision: {prec_rf:.4f}\")\n",
        "print(f\"  Recall:    {rec_rf:.4f}\")\n",
        "\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=le.classes_, digits=4))\n",
        "\n",
        "# 2. GRADIENT BOOSTING\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODELO 2: GRADIENT BOOSTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=100, max_depth=8, learning_rate=0.1,\n",
        "    random_state=42, verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nValidación cruzada (3-fold)...\")\n",
        "cv_scores_gb = cross_val_score(gb, X_train_balanced, y_train_balanced, cv=3, scoring='f1_weighted')\n",
        "print(f\"CV F1-Weighted: {cv_scores_gb.mean():.4f} (+/- {cv_scores_gb.std():.4f})\")\n",
        "\n",
        "print(\"\\nEntrenando modelo completo...\")\n",
        "gb.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"\\nPrediciendo en TEST...\")\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "y_pred_proba_gb = gb.predict_proba(X_test)\n",
        "\n",
        "acc_gb = accuracy_score(y_test, y_pred_gb)\n",
        "f1_gb = f1_score(y_test, y_pred_gb, average='weighted')\n",
        "prec_gb = precision_score(y_test, y_pred_gb, average='weighted')\n",
        "rec_gb = recall_score(y_test, y_pred_gb, average='weighted')\n",
        "\n",
        "print(\"\\nResultados Gradient Boosting:\")\n",
        "print(f\"  Accuracy:  {acc_gb:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_gb:.4f}\")\n",
        "print(f\"  Precision: {prec_gb:.4f}\")\n",
        "print(f\"  Recall:    {rec_gb:.4f}\")\n",
        "\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred_gb, target_names=le.classes_, digits=4))\n",
        "\n",
        "# COMPARACIÓN DE MODELOS\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARACIÓN DE MODELOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparacion = pd.DataFrame({\n",
        "    'Random Forest': [acc_rf, f1_rf, prec_rf, rec_rf],\n",
        "    'Gradient Boosting': [acc_gb, f1_gb, prec_gb, rec_gb]\n",
        "}, index=['Accuracy', 'F1-Score', 'Precision', 'Recall'])\n",
        "\n",
        "print(\"\\n\" + comparacion.to_string())\n",
        "\n",
        "mejor_modelo = 'Random Forest' if acc_rf > acc_gb else 'Gradient Boosting'\n",
        "print(f\"\\nMejor modelo: {mejor_modelo}\")\n",
        "\n",
        "# MATRIZ DE CONFUSIÓN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MATRICES DE CONFUSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "axes[0].set_title('Random Forest')\n",
        "axes[0].set_ylabel('Real')\n",
        "axes[0].set_xlabel('Predicción')\n",
        "\n",
        "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
        "sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "axes[1].set_title('Gradient Boosting')\n",
        "axes[1].set_ylabel('Real')\n",
        "axes[1].set_xlabel('Predicción')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# IMPORTANCIA DE FEATURES (Random Forest)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 20 FEATURES MAS IMPORTANTES (Random Forest)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': final_cols,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n\" + feature_importance.head(20).to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "feature_importance.head(20).sort_values('Importance').plot(\n",
        "    kind='barh', x='Feature', y='Importance', ax=ax, color='steelblue'\n",
        ")\n",
        "ax.set_title('Top 20 Features Más Importantes', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Importancia')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# GUARDAR MODELOS\n",
        "\n",
        "print(\"\\nGuardando modelos entrenados...\")\n",
        "\n",
        "with open(\"random_forest_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(rf, f)\n",
        "\n",
        "with open(\"gradient_boosting_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(gb, f)\n",
        "\n",
        "print(\"\\nEntrenamiento y evaluación completados\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "Gw2CRH220Evc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CARGAR MODELO Y PREPROCESADOR\n",
        "\n",
        "print(\"Cargando modelo y preprocesador...\\n\")\n",
        "\n",
        "# Cargar modelo entrenado (Gradient Boosting)\n",
        "with open(\"gradient_boosting_model.pkl\", \"rb\") as f:\n",
        "    modelo = pickle.load(f)\n",
        "\n",
        "# Cargar label encoder\n",
        "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
        "    le = pickle.load(f)\n",
        "\n",
        "# Cargar preprocessor\n",
        "with open(\"preprocessor.pkl\", \"rb\") as f:\n",
        "    preprocessor = pickle.load(f)\n",
        "\n",
        "print(\"Modelo cargado\")\n",
        "print(f\"Clases: {le.classes_}\")\n",
        "\n",
        "# CARGAR DATASET ORIGINAL SIN SAMPLEAR\n",
        "\n",
        "print(\"\\nCargando dataset original sin samplear...\\n\")\n",
        "\n",
        "test_original = test.copy()\n",
        "\n",
        "print(f\"Test original shape: {test_original.shape}\")\n",
        "print(\"Expected rows: 296,786\")\n",
        "\n",
        "if len(test_original) != 296786:\n",
        "    print(f\"ADVERTENCIA: Se esperaban 296,786 filas pero se cargaron {len(test_original):,}\")\n",
        "\n",
        "# PREPROCESAR DATASET COMPLETO\n",
        "\n",
        "print(\"\\nPreprocesando dataset completo...\\n\")\n",
        "inicio = time.time()\n",
        "\n",
        "ID_COL = \"ID\"\n",
        "TARGET = \"RENDIMIENTO_GLOBAL\"\n",
        "NUM_VARS = ['PERIODO_ACADEMICO', 'INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']\n",
        "ESTRATO_MAP = {f'Estrato {i}': i for i in range(1, 7)}\n",
        "REVERSE_ESTRATO = {v: k for k, v in ESTRATO_MAP.items()}\n",
        "\n",
        "test_proc = test_original.copy()\n",
        "\n",
        "# 1. Limpiar duplicadas\n",
        "cols_drop = [c for c in test_proc.columns if \".1\" in c]\n",
        "test_proc.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "\n",
        "cat_vars = [c for c in test_proc.columns if c not in NUM_VARS + [ID_COL]]\n",
        "\n",
        "print(\"Columnas duplicadas eliminadas\")\n",
        "\n",
        "# 2. Imputar F_ESTRATOVIVIENDA\n",
        "if \"F_ESTRATOVIVIENDA\" in test_proc.columns:\n",
        "    socio_vars = [\n",
        "        'F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR', 'F_TIENELAVADORA',\n",
        "        'F_TIENEINTERNET', 'F_EDUCACIONPADRE', 'F_EDUCACIONMADRE',\n",
        "        'E_VALORMATRICULAUNIVERSIDAD'\n",
        "    ]\n",
        "    socio_vars = [v for v in socio_vars if v in test_proc.columns]\n",
        "\n",
        "    temp = test_proc[socio_vars + [\"F_ESTRATOVIVIENDA\"]].copy()\n",
        "    for col in socio_vars:\n",
        "        temp[col] = temp[col].astype(\"category\").cat.codes.replace({-1: np.nan})\n",
        "\n",
        "    temp[\"F_ESTRATOVIVIENDA\"] = temp[\"F_ESTRATOVIVIENDA\"].map(ESTRATO_MAP)\n",
        "\n",
        "    from sklearn.impute import KNNImputer\n",
        "    imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
        "    imputado = np.round(imputer.fit_transform(temp)[:, -1]).clip(1, 6)\n",
        "    test_proc[\"F_ESTRATOVIVIENDA\"] = pd.Series(imputado, index=test_proc.index).map(REVERSE_ESTRATO)\n",
        "\n",
        "    del temp, imputer, imputado\n",
        "\n",
        "print(\"F_ESTRATOVIVIENDA imputado\")\n",
        "\n",
        "# 3. Imputar categóricas\n",
        "for col in cat_vars:\n",
        "    if test_proc[col].isnull().sum() > 0:\n",
        "        fill_value = test_proc[col].mode()[0] if test_proc[col].nunique() <= 3 else \"Desconocido\"\n",
        "        test_proc[col] = test_proc[col].fillna(fill_value)\n",
        "\n",
        "print(\"Categóricas imputadas\")\n",
        "\n",
        "# 4. Capping outliers\n",
        "for col in NUM_VARS:\n",
        "    p1, p99 = test_proc[col].quantile([0.01, 0.99])\n",
        "    test_proc[col] = test_proc[col].clip(p1, p99)\n",
        "\n",
        "print(\"Outliers cappeados\")\n",
        "\n",
        "# APLICAR TRANSFORMACIONES\n",
        "\n",
        "print(\"\\nAplicando transformaciones...\\n\")\n",
        "\n",
        "X_test = test_proc[NUM_VARS + cat_vars]\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"Transformaciones aplicadas: {X_test_processed.shape}\")\n",
        "\n",
        "# HACER PREDICCIONES\n",
        "print(\"\\nHaciendo predicciones...\\n\")\n",
        "\n",
        "# Predicciones (clases)\n",
        "y_pred_encoded = modelo.predict(X_test_processed)\n",
        "\n",
        "# Convertir códigos a clases\n",
        "y_pred = le.inverse_transform(y_pred_encoded)\n",
        "\n",
        "print(f\"Predicciones completadas: {len(y_pred)} muestras\")"
      ],
      "metadata": {
        "id": "Vxr_TuKIAygI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREAR DATAFRAME DE ENVÍO\n",
        "print(\"\\nCreando archivo de envío...\\n\")\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_proc[ID_COL].values,\n",
        "    'RENDIMIENTO_GLOBAL': y_pred\n",
        "})\n",
        "\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "print(f\"Filas: {len(submission):,}\")\n",
        "print(f\"Columnas: {list(submission.columns)}\")\n",
        "\n",
        "# ESTADÍSTICAS\n",
        "\n",
        "print(\"\\nDistribución de predicciones:\")\n",
        "pred_dist = pd.Series(y_pred).value_counts()\n",
        "for cls in le.classes_:\n",
        "    count = (y_pred == cls).sum()\n",
        "    pct = count / len(y_pred) * 100\n",
        "    print(f\"{cls}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# GUARDAR\n",
        "print(\"\\nGuardando submission.csv...\\n\")\n",
        "\n",
        "archivo = 'submission.csv'\n",
        "submission.to_csv(archivo, index=False)\n",
        "\n",
        "print(f\"{archivo} guardado ({len(submission):,} filas)\")\n",
        "\n",
        "# VERIFICACIÓN\n",
        "\n",
        "if len(submission) == 296786:\n",
        "    print(\"CORRECTO: El archivo tiene exactamente 296,786 filas\")\n",
        "else:\n",
        "    print(f\"ERROR: El archivo tiene {len(submission):,} filas en lugar de 296,786\")\n",
        "\n",
        "tiempo_total = time.time() - inicio\n",
        "print(f\"Tiempo total: {tiempo_total/60:.2f} minutos\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "uDII96onG4J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia -f submission.csv -m \"Gradient Boosting\""
      ],
      "metadata": {
        "id": "HZx_r-LlFcpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}