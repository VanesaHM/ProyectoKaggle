{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSLk9H4+jAbDoIYS19D27f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanesaHM/ProyectoKaggle/blob/main/05_modelo_con_preprocesado_(KNNImputer%2C_OHE%2C_StandardScaler%2C_SMOTE)_y_Red_Neuronal_(Keras).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UDEA/ai4eng 20252 - Pruebas Saber Pro Colombia**\n",
        "\n",
        "Crear un modelo para predecir el rendimiento de los estudiantes en las pruebas Saber Pro"
      ],
      "metadata": {
        "id": "5G8Cr83a-mRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descripción general**\n",
        "\n",
        "Las Pruebas Saber Pro son exámenes estandarizados que se administran en Colombia para evaluar la calidad y el nivel de conocimiento y competencias de los estudiantes de educación superior, es decir, de instituciones de educación superior como universidades y tecnológicos. Estas pruebas son parte de los esfuerzos del Gobierno de Colombia para monitorear y mejorar la calidad de la educación superior en el país.\n",
        "\n",
        "Estas Pruebas constan cinco componentes genéricos, Inglés, Lectura Crítica, Competencias Ciudadanas, Razonamiento Cuantitativo y Comunicación Escrita.\n",
        "\n",
        "Tu tarea será crear un modelo de clasificación que para cada estudiante prediga qué desempeño va a tener: bajo, medio-bajo, medio-alto o alto."
      ],
      "metadata": {
        "id": "hmMM0_ze-7Js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descripción**\n",
        "\n",
        "El conjunto de datos contiene más de 50 columnas que describen de manera distintos aspectos de cada estudiante, incluyendo:\n",
        "\n",
        "Información socieconómica: Describen características socieconómicas del estudiante como su estrato, educación de sus padres, estrato, entre otras.\n",
        "\n",
        "Información de instituciones: Describen las instituciones de donde provienen los estudiantes.\n",
        "\n",
        "Información del estudiante: Describe particularidades del estudiante como su edad, que programa estudian, la modalidad de estudio, etc.\n",
        "\n",
        "Información estadística: Describe algunos coeficientes que equipos de estudio han desarrollado que podría ayudar a la clasificación.\n",
        "\n",
        "Así como muchos otros datos que ayudan a clasificar de manera precisa los niveles de desempeño"
      ],
      "metadata": {
        "id": "zemYOiZe_Rt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **modelo con preprocesado (KNNImputer, OHE, StandardScaler, SMOTE) y Red Neuronal (Keras)**\n",
        "\n"
      ],
      "metadata": {
        "id": "73F_K802SKfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Importar librerías**"
      ],
      "metadata": {
        "id": "qxkxGgLfBau0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsxYTe_M-W6k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    f1_score\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import pickle\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Configuración visual**"
      ],
      "metadata": {
        "id": "4unuiIErBmcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\")"
      ],
      "metadata": {
        "id": "3y_7ELf0BrQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Cargar los datos**"
      ],
      "metadata": {
        "id": "mCLCJ-2SB2gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "eauNGNJSUufQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n",
        "!chmod 600 ./kaggle.json\n",
        "!kaggle competitions download -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia"
      ],
      "metadata": {
        "id": "34zmsVf0B4SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Descomprimir los datos**"
      ],
      "metadata": {
        "id": "Uf19pAiiCSZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip udea*.zip > /dev/null"
      ],
      "metadata": {
        "id": "T-BrCfI9Cati"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test  = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "hxSykEUtCw8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANÁLISIS EXPLORATORIO INICIAL**\n",
        "\n",
        "Se muestran las primeras filas, tipos y valores nulos."
      ],
      "metadata": {
        "id": "DDa3yhzXDGBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train is not None:\n",
        "    display(train.head())\n",
        "    display(train.info())\n",
        "    display(train.isna().sum().sort_values(ascending=False).head(20))\n",
        "else:\n",
        "    print('No train available to display.')"
      ],
      "metadata": {
        "id": "BcoLv-_SWUD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPROCESAMIENTO**"
      ],
      "metadata": {
        "id": "vpPimPFNd1yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"PREPROCESAMIENTO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_processed = train.copy()\n",
        "inicio_total = time.time()\n",
        "\n",
        "ID_COL = \"ID\"\n",
        "TARGET = \"RENDIMIENTO_GLOBAL\"\n",
        "NUM_VARS = ['PERIODO_ACADEMICO', 'INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']\n",
        "ESTRATO_MAP = {f'Estrato {i}': i for i in range(1, 7)}\n",
        "REVERSE_ESTRATO = {v: k for k, v in ESTRATO_MAP.items()}\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "SAMPLE_FRAC = 0.5\n",
        "\n",
        "print(f\"\\nFilas originales: {len(train_processed):,}\")\n",
        "\n",
        "# SAMPLING\n",
        "if len(train_processed) > 200000:\n",
        "    print(f\"Stratified sampling al {SAMPLE_FRAC*100:.0f}%...\")\n",
        "    train_sample, _ = train_test_split(\n",
        "        train_processed, train_size=SAMPLE_FRAC, random_state=RANDOM_STATE,\n",
        "        stratify=train_processed[TARGET]\n",
        "    )\n",
        "    train_processed = train_sample.reset_index(drop=True)\n",
        "    print(f\"Filas después: {len(train_processed):,}\")\n",
        "\n",
        "# LIMPIEZA\n",
        "cols_drop = [c for c in train_processed.columns if \".1\" in c]\n",
        "train_processed.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "cat_vars = [c for c in train_processed.columns if c not in NUM_VARS + [ID_COL, TARGET]]\n",
        "\n",
        "print(f\"Numéricas: {len(NUM_VARS)} | Categóricas: {len(cat_vars)}\")\n",
        "\n",
        "# IMPUTACIÓN F_ESTRATOVIVIENDA\n",
        "if \"F_ESTRATOVIVIENDA\" in train_processed.columns:\n",
        "    socio_vars = [\n",
        "        'F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR', 'F_TIENELAVADORA',\n",
        "        'F_TIENEINTERNET', 'F_EDUCACIONPADRE', 'F_EDUCACIONMADRE',\n",
        "        'E_VALORMATRICULAUNIVERSIDAD'\n",
        "    ]\n",
        "    socio_vars = [v for v in socio_vars if v in train_processed.columns]\n",
        "\n",
        "    temp = train_processed[socio_vars + [\"F_ESTRATOVIVIENDA\"]].copy()\n",
        "    for col in socio_vars:\n",
        "        temp[col] = temp[col].astype(\"category\").cat.codes.replace({-1: np.nan})\n",
        "\n",
        "    temp[\"F_ESTRATOVIVIENDA\"] = temp[\"F_ESTRATOVIVIENDA\"].map(ESTRATO_MAP)\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
        "    imputado = np.round(imputer.fit_transform(temp)[:, -1]).clip(1, 6)\n",
        "    train_processed[\"F_ESTRATOVIVIENDA\"] = pd.Series(imputado, index=train_processed.index).map(REVERSE_ESTRATO)\n",
        "    del temp, imputer, imputado\n",
        "\n",
        "# IMPUTACIÓN CATEGÓRICAS\n",
        "for col in cat_vars:\n",
        "    if train_processed[col].isnull().sum() > 0:\n",
        "        fill_value = train_processed[col].mode()[0] if train_processed[col].nunique() <= 3 else \"Desconocido\"\n",
        "        train_processed[col] = train_processed[col].fillna(fill_value)\n",
        "\n",
        "print(\"Imputación completada\")\n",
        "\n",
        "# OUTLIERS\n",
        "for col in NUM_VARS:\n",
        "    p1, p99 = train_processed[col].quantile([0.01, 0.99])\n",
        "    train_processed[col] = train_processed[col].clip(p1, p99)\n",
        "\n",
        "print(\"Outliers cappeados\")\n",
        "\n",
        "# FEATURE ENGINEERING\n",
        "print(\"\\nFEATURE ENGINEERING:\")\n",
        "\n",
        "train_processed['PROMEDIO_INDICADORES'] = train_processed[['INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']].mean(axis=1)\n",
        "train_processed['STD_INDICADORES'] = train_processed[['INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']].std(axis=1)\n",
        "\n",
        "max_ind = train_processed[['INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']].max(axis=1)\n",
        "min_ind = train_processed[['INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']].min(axis=1)\n",
        "train_processed['RANGO_INDICADORES'] = max_ind - min_ind\n",
        "\n",
        "activos = ['F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR', 'F_TIENELAVADORA', 'F_TIENEINTERNET']\n",
        "activos_disponibles = [a for a in activos if a in train_processed.columns]\n",
        "\n",
        "def contar_activos(row):\n",
        "    return sum(1 for col in activos_disponibles if row.get(col) == 'Si')\n",
        "\n",
        "train_processed['NUM_ACTIVOS'] = train_processed[activos_disponibles].apply(contar_activos, axis=1)\n",
        "\n",
        "if 'E_HORASSEMANATRABAJA' in train_processed.columns:\n",
        "    horas_map = {'0': 0, 'Menos de 10': 5, 'Entre 10 y 20': 15,\n",
        "                 'Entre 20 y 30': 25, 'Más de 30 horas': 35}\n",
        "    train_processed['HORAS_TRABAJO_NUM'] = train_processed['E_HORASSEMANATRABAJA'].map(horas_map).fillna(0)\n",
        "    train_processed['INDICADORES_X_TRABAJO'] = train_processed['PROMEDIO_INDICADORES'] / (train_processed['HORAS_TRABAJO_NUM'] + 1)\n",
        "\n",
        "educacion_map = {\n",
        "    'Primaria incompleta': 1, 'Primaria completa': 2,\n",
        "    'Secundaria incompleta': 2.5, 'Secundaria completa': 3,\n",
        "    'Educación profesional incompleta': 3.5, 'Educación profesional completa': 4,\n",
        "    'Postgrado': 5\n",
        "}\n",
        "\n",
        "if 'F_EDUCACIONPADRE' in train_processed.columns and 'F_EDUCACIONMADRE' in train_processed.columns:\n",
        "    padre = train_processed['F_EDUCACIONPADRE'].map(educacion_map).fillna(2)\n",
        "    madre = train_processed['F_EDUCACIONMADRE'].map(educacion_map).fillna(2)\n",
        "    train_processed['EDUCACION_PADRES_SCORE'] = (padre + madre) / 2\n",
        "\n",
        "print(\"Feature engineering completado\")\n",
        "\n",
        "NUM_VARS_NEW = NUM_VARS + [\n",
        "    'PROMEDIO_INDICADORES', 'STD_INDICADORES', 'RANGO_INDICADORES',\n",
        "    'NUM_ACTIVOS', 'HORAS_TRABAJO_NUM', 'INDICADORES_X_TRABAJO',\n",
        "    'EDUCACION_PADRES_SCORE'\n",
        "]\n",
        "NUM_VARS_NEW = [v for v in NUM_VARS_NEW if v in train_processed.columns]\n",
        "\n",
        "print(f\"\\nFeatures numéricas totales: {len(NUM_VARS_NEW)}\")\n",
        "\n",
        "# TRAIN/TEST SPLIT\n",
        "X = train_processed[NUM_VARS_NEW + cat_vars]\n",
        "y = train_processed[TARGET]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "del train_processed, X, y\n",
        "print(f\"Train/Test split: {len(X_train):,} / {len(X_test):,}\")\n",
        "\n",
        "# TRANSFORMACIONES\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), NUM_VARS_NEW),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, max_categories=50), cat_vars)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "cat_ohe_cols = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(cat_vars)\n",
        "final_cols = NUM_VARS_NEW + list(cat_ohe_cols)\n",
        "\n",
        "print(f\"Features finales: {len(final_cols)}\")\n",
        "\n",
        "# CODIFICAR TARGET\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "print(f\"Classes: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
        "\n",
        "# BALANCEAR CON SMOTE\n",
        "class_counts = np.bincount(y_train_encoded)\n",
        "imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "\n",
        "if imbalance_ratio > 2.5:\n",
        "    print(f\"Aplicando SMOTE (ratio: {imbalance_ratio:.2f})...\")\n",
        "    smote = SMOTE(random_state=RANDOM_STATE, n_jobs=-1, k_neighbors=3, sampling_strategy='not majority')\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train_encoded)\n",
        "else:\n",
        "    print(f\"Balanceo aceptable (ratio: {imbalance_ratio:.2f})\")\n",
        "    X_train_balanced, y_train_balanced = X_train_processed, y_train_encoded\n",
        "\n",
        "print(\"\\nPREPROCESAMIENTO COMPLETADO\\n\")"
      ],
      "metadata": {
        "id": "p_XlDoz16zPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENTRENAMIENTO CON NEURAL NETWORK**"
      ],
      "metadata": {
        "id": "9x7z-Hcq0DG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir sparse a dense para Keras\n",
        "if hasattr(X_train_balanced, 'toarray'):\n",
        "    X_train_balanced = X_train_balanced.toarray()\n",
        "    X_test_processed = X_test_processed.toarray()\n",
        "\n",
        "print(\"\\nNEURAL NETWORK ARCHITECTURE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Crear modelo\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(X_train_balanced.shape[1],)),\n",
        "\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "# Entrenar\n",
        "print(\"\\nEntrenando Neural Network...\")\n",
        "history = model.fit(\n",
        "    X_train_balanced, y_train_balanced,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Entrenamiento completado\")\n",
        "\n",
        "# PREDICCIONES\n",
        "print(\"\\nPrediciendo en TEST...\")\n",
        "y_pred_proba = model.predict(X_test_processed, verbose=0)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "# MÉTRICAS\n",
        "acc_nn = accuracy_score(y_test_encoded, y_pred)\n",
        "f1_nn = f1_score(y_test_encoded, y_pred, average='weighted')\n",
        "\n",
        "print(\"\\nRESULTADOS NEURAL NETWORK:\")\n",
        "print(f\"  Accuracy:  {acc_nn:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_nn:.4f}\")\n",
        "\n",
        "print(\"\\nREPORTE DE CLASIFICACIÓN:\")\n",
        "print(classification_report(y_test_encoded, y_pred, target_names=le.classes_, digits=4))\n",
        "\n",
        "# GRÁFICAS\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history.history['loss'], label='Train Loss')\n",
        "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
        "axes[0].set_title('Loss - Neural Network')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid()\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "axes[1].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "axes[1].set_title('Accuracy - Neural Network')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# MATRIZ DE CONFUSIÓN\n",
        "cm = confusion_matrix(y_test_encoded, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', ax=ax,\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "ax.set_title('Matriz de Confusión - Neural Network')\n",
        "ax.set_ylabel('Real')\n",
        "ax.set_xlabel('Predicción')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# GUARDAR MODELO\n",
        "print(\"\\nGuardando modelo...\")\n",
        "model.save('neural_network_model.h5')\n",
        "with open(\"preprocessor_nn.pkl\", \"wb\") as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "with open(\"label_encoder_nn.pkl\", \"wb\") as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "print(\"ENTRENAMIENTO COMPLETADO\\n\")\n"
      ],
      "metadata": {
        "id": "eU5d_9c_NLyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREDICCIÓN EN DATASET COMPLETO**"
      ],
      "metadata": {
        "id": "e6t68_1JNcWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCargando dataset original sin samplear...\")\n",
        "test_proc = test.copy()\n",
        "\n",
        "print(f\"Test original shape: {test_proc.shape}\")\n",
        "\n",
        "# PREPROCESAR TEST (igual que train)\n",
        "cols_drop = [c for c in test_proc.columns if \".1\" in c]\n",
        "test_proc.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "\n",
        "if \"F_ESTRATOVIVIENDA\" in test_proc.columns:\n",
        "    socio_vars = [v for v in socio_vars if v in test_proc.columns]\n",
        "    temp = test_proc[socio_vars + [\"F_ESTRATOVIVIENDA\"]].copy()\n",
        "    for col in socio_vars:\n",
        "        temp[col] = temp[col].astype(\"category\").cat.codes.replace({-1: np.nan})\n",
        "    temp[\"F_ESTRATOVIVIENDA\"] = temp[\"F_ESTRATOVIVIENDA\"].map(ESTRATO_MAP)\n",
        "\n",
        "    from sklearn.impute import KNNImputer\n",
        "    imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
        "    imputado = np.round(imputer.fit_transform(temp)[:, -1]).clip(1, 6)\n",
        "    test_proc[\"F_ESTRATOVIVIENDA\"] = pd.Series(imputado, index=test_proc.index).map(REVERSE_ESTRATO)\n",
        "    del temp, imputer, imputado\n",
        "\n",
        "for col in cat_vars:\n",
        "    if test_proc[col].isnull().sum() > 0:\n",
        "        fill_value = test_proc[col].mode()[0] if test_proc[col].nunique() <= 3 else \"Desconocido\"\n",
        "        test_proc[col] = test_proc[col].fillna(fill_value)\n",
        "\n",
        "for col in NUM_VARS_NEW[:5]:\n",
        "    p1, p99 = test_proc[col].quantile([0.01, 0.99])\n",
        "    test_proc[col] = test_proc[col].clip(p1, p99)\n",
        "\n",
        "# Feature engineering en test\n",
        "test_proc['PROMEDIO_INDICADORES'] = test_proc[['INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']].mean(axis=1)\n",
        "test_proc['STD_INDICADORES'] = test_proc[['INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']].std(axis=1)\n",
        "test_proc['RANGO_INDICADORES'] = (\n",
        "    test_proc[['INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']].max(axis=1) -\n",
        "    test_proc[['INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4']].min(axis=1)\n",
        ")\n",
        "test_proc['NUM_ACTIVOS'] = test_proc[activos_disponibles].apply(contar_activos, axis=1)\n",
        "\n",
        "if 'E_HORASSEMANATRABAJA' in test_proc.columns:\n",
        "    test_proc['HORAS_TRABAJO_NUM'] = test_proc['E_HORASSEMANATRABAJA'].map(horas_map).fillna(0)\n",
        "    test_proc['INDICADORES_X_TRABAJO'] = test_proc['PROMEDIO_INDICADORES'] / (test_proc['HORAS_TRABAJO_NUM'] + 1)\n",
        "\n",
        "padre = test_proc['F_EDUCACIONPADRE'].map(educacion_map).fillna(2)\n",
        "madre = test_proc['F_EDUCACIONMADRE'].map(educacion_map).fillna(2)\n",
        "test_proc['EDUCACION_PADRES_SCORE'] = (padre + madre) / 2\n",
        "\n",
        "print(\"Test preprocesado y feature engineering aplicado\")\n",
        "\n",
        "# TRANSFORMAR\n",
        "X_test_full = test_proc[NUM_VARS_NEW + cat_vars]\n",
        "X_test_full_processed = preprocessor.transform(X_test_full)\n",
        "\n",
        "if hasattr(X_test_full_processed, 'toarray'):\n",
        "    X_test_full_processed = X_test_full_processed.toarray()\n",
        "\n",
        "print(f\"Transformaciones aplicadas: {X_test_full_processed.shape}\")\n",
        "\n",
        "# PREDECIR\n",
        "print(f\"\\nHaciendo predicciones en {len(X_test_full):,} muestras...\")\n",
        "y_pred_proba_full = model.predict(X_test_full_processed, verbose=0)\n",
        "y_pred_full = le.inverse_transform(np.argmax(y_pred_proba_full, axis=1))\n",
        "\n",
        "print(\"Predicciones completadas\")\n"
      ],
      "metadata": {
        "id": "liZOhnRbNc6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREAR SUBMISSION**"
      ],
      "metadata": {
        "id": "SBKL3YdsNtlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'ID': test_proc[ID_COL].values,\n",
        "    'RENDIMIENTO_GLOBAL': y_pred_full\n",
        "})\n",
        "\n",
        "print(\"\\nDistribución de predicciones:\")\n",
        "for cls in le.classes_:\n",
        "    count = (y_pred_full == cls).sum()\n",
        "    pct = count / len(y_pred_full) * 100\n",
        "    print(f\"  {cls}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Guardar archivo\n",
        "print(\"\\nGuardando submission.csv...\")\n",
        "submission.to_csv('submission_neural_network.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"Archivo: submission_neural_network.csv\")\n",
        "print(f\"Filas: {len(submission):,}\")\n",
        "print(f\"Columnas: {list(submission.columns)}\")\n",
        "\n",
        "print(\"\\n\" + submission.head(10).to_string(index=False))\n",
        "\n",
        "tiempo_total = time.time() - inicio_total\n",
        "print(f\"\\nTiempo total: {tiempo_total/60:.2f} minutos\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "Nb85xHTtNp4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia -f submission_neural_network.csv -m \"NEURAL NETWORK\""
      ],
      "metadata": {
        "id": "HZx_r-LlFcpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}